{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Add the parent directory to the system path\n",
    "sys.path.append(\"../04_survival_models/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import pprint\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import joblib\n",
    "import kaplanmeier as km\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from azureml.core import Dataset, Workspace\n",
    "from lifelines.statistics import logrank_test\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    KFold,\n",
    "    ParameterGrid,\n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sksurv.functions import StepFunction\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sksurv.metrics import (\n",
    "    concordance_index_censored,\n",
    "    concordance_index_ipcw,\n",
    "    cumulative_dynamic_auc,\n",
    "    integrated_brier_score,\n",
    ")\n",
    "from sksurv.nonparametric import kaplan_meier_estimator\n",
    "from uc2_functions import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1693554723785
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to fine-tune a Cox model using GRANT features as the baseline and to tune a RandomSurvivalForest with all available features as the feature selector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1693554724271
    }
   },
   "outputs": [],
   "source": [
    "# Legend\n",
    "PATH_LEGEND = \"Legenda_Variabili_Uri_Larcher.xlsx\"\n",
    "# Directories\n",
    "DIR_SC = os.path.join(os.path.dirname(os.getcwd()), \"sc\")  # Legend\n",
    "DIR_MODEL_JSON = \"../models_json\"  # Parameters for the models not used during inference\n",
    "DIR_MODEL_PKL = \"../models_pkl\"  # Weights for the models used during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "EXPERIMENT_NAME = \"UC2_larcher_2024_02\"\n",
    "PARENT_RUN_ID = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1693554728157
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# azureml-core of version 1.0.72 or higher is required\n",
    "# azureml-dataprep[pandas] of version 1.1.34 or higher is required\n",
    "\n",
    "subscription_id = \"753a0b42-95dc-4871-b53e-160ceb0e6bc1\"\n",
    "resource_group = \"rg-s-race-aml-dev-we\"\n",
    "workspace_name = \"amlsraceamldevwe01\"\n",
    "\n",
    "workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
    "\n",
    "dataset = Dataset.get_by_name(workspace, name=\"UC2_larcher_survival_csm_ohe_5yrs\")\n",
    "df_ohe = dataset.to_pandas_dataframe()\n",
    "print(df_ohe.shape)\n",
    "df_ohe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreate the schema from tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1693554728429
    }
   },
   "outputs": [],
   "source": [
    "tags = dataset.tags\n",
    "\n",
    "dtypes = json.loads(tags[\"dtypes_json\"])\n",
    "is_ordinal = json.loads(tags[\"is_ordinal_json\"])\n",
    "\n",
    "for col in dtypes.keys():\n",
    "    if dtypes[col] == \"category\":\n",
    "        categories = (\n",
    "            sorted(df_ohe[col].dropna().unique())\n",
    "            if is_ordinal[col]\n",
    "            else df_ohe[col].dropna().unique()\n",
    "        )\n",
    "        df_ohe[col] = pd.Categorical(\n",
    "            df_ohe[col], categories=categories, ordered=is_ordinal[col]\n",
    "        )\n",
    "    else:\n",
    "        df_ohe[col] = df_ohe[col].astype(dtypes[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1693554728660
    }
   },
   "outputs": [],
   "source": [
    "count_columns_by_dtype(df_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `.xlsx` Legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1693554728980
    }
   },
   "outputs": [],
   "source": [
    "df_legend = pd.read_excel(\n",
    "    os.path.join(DIR_SC, PATH_LEGEND), sheet_name=\"Legenda Urologi - DB Larcher\"\n",
    ")\n",
    "# Replace dot with underscore\n",
    "df_legend[\"Variable\"] = df_legend[\"Variable\"].apply(lambda x: x.replace(\".\", \"_\"))\n",
    "# Forward fill domain\n",
    "df_legend[\"Domain\"] = df_legend[\"Domain\"].fillna(method=\"ffill\")\n",
    "df_legend.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1693554729363
    }
   },
   "outputs": [],
   "source": [
    "dict_legend = pd.Series(\n",
    "    df_legend[\"Definition\"].values, index=df_legend[\"Variable\"]\n",
    ").to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start mlflow run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "mlflow.start_run(run_name=str(RANDOM_STATE))\n",
    "if PARENT_RUN_ID:\n",
    "    mlflow.set_tag(\"parent_run_id\", PARENT_RUN_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop na on target columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1693554729567
    }
   },
   "outputs": [],
   "source": [
    "not_features = [\"P_1_id\", \"death\", \"csm\", \"ocm\", \"ttdeath\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1693554729802
    }
   },
   "outputs": [],
   "source": [
    "print(df_ohe.shape[0])\n",
    "df_ohe = df_ohe.dropna(subset=[\"ttdeath\", \"death\"])\n",
    "print(df_ohe.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Censoring rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be impossible to have ocm if ttdeath is < 60\n",
    "df_ohe[df_ohe[\"ttdeath\"] < 60][\"ocm\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1693554731092
    }
   },
   "outputs": [],
   "source": [
    "# Distribution of ttdeath\n",
    "df_ohe[\"ttdeath\"].hist(bins=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of ttdeath, exluding 60\n",
    "df_ohe[df_ohe[\"ttdeath\"] < 60][\"ttdeath\"].hist(bins=59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of ttdeath by binary class, exluding 60\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "# Plot 1: death == False\n",
    "ax[0].hist(df_ohe[(df_ohe[\"ttdeath\"] < 60) & (df_ohe[\"death\"] == False)][\"ttdeath\"], bins=60)\n",
    "ax[0].set_title('Non morti + censurati')\n",
    "ax[0].set_xlabel('ttdeath')\n",
    "ax[0].set_ylabel('Frequency')\n",
    "\n",
    "# Plot 2: death == True\n",
    "ax[1].hist(df_ohe[(df_ohe[\"ttdeath\"] < 60) & (df_ohe[\"death\"] == True)][\"ttdeath\"], bins=60)\n",
    "ax[1].set_title('Morti')\n",
    "ax[1].set_xlabel('ttdeath')\n",
    "ax[1].set_ylabel('Frequency')\n",
    "\n",
    "# General title\n",
    "fig.suptitle('DBURI Dataset', fontsize=16)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to fit the suptitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1693554731359
    }
   },
   "outputs": [],
   "source": [
    "# Censoring rate\n",
    "print(df_ohe[\"death\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Harrellâ€™s concordance index is known to be biased upwards if the amount of censoring in the test data is high. Uno et al proposed an alternative estimator of the concordance index that behaves better in such situations. Therefore, we are going to apply `concordance_index_ipcw` as the main metric and `concordance_index_censored` as a reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1693554731698
    }
   },
   "outputs": [],
   "source": [
    "features_all = sorted(set(df_ohe.columns.tolist()) - set(not_features))\n",
    "print(len(features_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1693554731873
    }
   },
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "X = df_ohe[features_all]\n",
    "y = np.array(\n",
    "    [(event, time) for event, time in zip(df_ohe[\"death\"], df_ohe[\"ttdeath\"])],\n",
    "    dtype=[(\"event\", bool), (\"time\", float)],\n",
    ")\n",
    "ids = df_ohe[\"P_1_id\"]\n",
    "mlflow.log_param(\n",
    "    \"death_perc_5yrs\",\n",
    "    pd.Series(y[\"event\"]).value_counts(sort=True, normalize=True)[True],\n",
    ")\n",
    "\n",
    "# Split data and IDs into training and testing sets\n",
    "(\n",
    "    X_train_missing,\n",
    "    X_test_missing,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    ids_train,\n",
    "    ids_test,\n",
    ") = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    ids,\n",
    "    test_size=0.2,\n",
    "    stratify=y[\"event\"],\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "del X, y, ids\n",
    "# Check distributions of death event on train and test\n",
    "print(pd.Series(y_train[\"event\"]).value_counts(sort=True, normalize=True))\n",
    "print(pd.Series(y_test[\"event\"]).value_counts(sort=True, normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and trasform on train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_missing.copy()\n",
    "\n",
    "imputer = IterativeImputer(\n",
    "    max_iter=25, initial_strategy=\"median\", random_state=RANDOM_STATE\n",
    ")\n",
    "imputer = imputer.fit(X_train)\n",
    "X_train = imputer.transform(X_train)\n",
    "X_train = pd.DataFrame(X_train, columns=X_train_missing.columns)\n",
    "\n",
    "# Assert\n",
    "assert set(X_train.columns) == set(X_train_missing.columns)\n",
    "\n",
    "del X_train_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test_missing.copy()\n",
    "\n",
    "X_test = imputer.transform(X_test)\n",
    "X_test = pd.DataFrame(X_test, columns=X_test_missing.columns)\n",
    "\n",
    "# Assert\n",
    "assert set(X_test.columns) == set(X_test_missing.columns)\n",
    "\n",
    "del X_test_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cox model - GRANT fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"CoxPHSurvivalAnalysis_grant_finetune_T1\"\n",
    "mlflow.start_run(run_name=model_name, nested=True)\n",
    "mlflow.log_param(\"random_state\", RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As baseline we train a Cox model trained on features from prognostic model GRANT (table Table 6.3 at https://uroweb.org/guidelines/renal-cell-carcinoma/chapter/prognostic-factors):\n",
    "\n",
    "1. AGE\n",
    "\n",
    "2. T classification\n",
    "\n",
    "3. N classification\n",
    "\n",
    "4. (Fuhrman) grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_grant = [\n",
    "    \"age\",\n",
    "    \"pT\",\n",
    "    \"pN_1_0\",\n",
    "    \"grade\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1693554732165
    }
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "cox_grant = CoxPHSurvivalAnalysis()\n",
    "cox_grant.fit(X_train[features_grant], y_train)\n",
    "mlflow.log_param(\"feature_names_in\", cox_grant.feature_names_in_)\n",
    "mlflow.log_param(\"n_features_in\", cox_grant.n_features_in_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model weights to pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights to pkl\n",
    "os.makedirs(DIR_MODEL_PKL, exist_ok=True)\n",
    "model_path = os.path.join(DIR_MODEL_PKL, \"larcher_{}_{}.pkl\".format(model_name, RANDOM_STATE))\n",
    "joblib.dump(cox_grant, model_path)\n",
    "mlflow.log_artifact(model_path)\n",
    "mlflow.log_param(\"model_path\", model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_censored, result_ipcw, score_brier, mean_auc, fig = validate_sksurv_model(model=cox_grant,\n",
    "                                                                                 y_train=y_train,\n",
    "                                                                                 X_test=X_test[features_grant],\n",
    "                                                                                 y_test=y_test,\n",
    "                                                                                 tau=60)\n",
    "print(\"concordance_index_censored\", round(result_censored, 3))\n",
    "mlflow.log_metric(\"concordance_index_censored\", result_censored)\n",
    "print(\"concordance_index_ipcw\", round(result_ipcw, 3))\n",
    "mlflow.log_metric(\"concordance_index_ipcw\", result_ipcw)\n",
    "print(\"integrated_brier_score\", round(score_brier, 3))\n",
    "mlflow.log_metric(\"integrated_brier_score\", score_brier)\n",
    "print(\"mean_cumulative_dynamic_auc\", round(mean_auc, 3))\n",
    "mlflow.log_metric(\"mean_cumulative_dynamic_auc\", mean_auc)\n",
    "mlflow.log_figure(fig, \"time_dependent_auc.png\")\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Survival Forest - T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"RandomSurvivalForest_selector_T1\"\n",
    "mlflow.start_run(run_name=model_name, nested=True)\n",
    "mlflow.log_param(\"random_state\", RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to obtain a ranking of important features at t1 to gain medical knowledge, and use it as imput to a Cox model (feature selection)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter search with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.log2(len(features_all)))\n",
    "print(np.sqrt(len(features_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1693554733231
    }
   },
   "outputs": [],
   "source": [
    "# Define search spaces\n",
    "grid = {\n",
    "    \"n_estimators\": (10, 100),\n",
    "    \"max_depth\": (2, 50),\n",
    "    \"min_samples_split\": (2, 50),\n",
    "    \"min_samples_leaf\": (2, 50),\n",
    "    \"max_features\": (8, min(50, len(features_all))),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter search with Optuna\n",
    "optimize_rsf(X_tune=X_train,\n",
    "             y_tune=y_train,\n",
    "             grid=grid,\n",
    "             n_trials=1000,\n",
    "             n_folds=10,\n",
    "             model_dir=DIR_MODEL_JSON,\n",
    "             model_filename=\"larcher_{}_{}.json\".format(model_name, RANDOM_STATE),\n",
    "             random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "file_path = os.path.join(\n",
    "    DIR_MODEL_JSON, \"larcher_{}_{}.json\".format(model_name, RANDOM_STATE)\n",
    ")\n",
    "\n",
    "# Read the JSON file to get the best hyperparameters\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    print(\"Log from best RandomSurvivalForest model:\")\n",
    "    pprint.pprint(data)\n",
    "    mlflow.log_params(data)\n",
    "    print()\n",
    "    print(\"Grid:\")\n",
    "    pprint.pprint(grid)\n",
    "    assert data[\"random_state\"] == RANDOM_STATE\n",
    "    mlflow.log_params(grid)\n",
    "\n",
    "# Create a new RandomSurvivalForest instance with the best parameters\n",
    "rsf_best_t1 = RandomSurvivalForest(**data[\"best_params\"], random_state=RANDOM_STATE)\n",
    "\n",
    "# Fit the model on the complete training data\n",
    "rsf_best_t1.fit(X_train, y_train)\n",
    "if len(rsf_best_t1.feature_names_in_) < 50:\n",
    "    mlflow.log_param(\"feature_names_in\", rsf_best_t1.feature_names_in_)\n",
    "else:\n",
    "    mlflow.log_param(\"feature_names_in\", \"More than 50 features\")\n",
    "mlflow.log_param(\"n_features_in\", rsf_best_t1.n_features_in_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1693554733902
    }
   },
   "outputs": [],
   "source": [
    "result_censored, result_ipcw, score_brier, mean_auc, fig = validate_sksurv_model(model=rsf_best_t1,\n",
    "                                                                                 y_train=y_train,\n",
    "                                                                                 X_test=X_test,\n",
    "                                                                                 y_test=y_test,\n",
    "                                                                                 tau=60)\n",
    "print(\"concordance_index_censored\", round(result_censored, 3))\n",
    "mlflow.log_metric(\"concordance_index_censored\", result_censored)\n",
    "print(\"concordance_index_ipcw\", round(result_ipcw, 3))\n",
    "mlflow.log_metric(\"concordance_index_ipcw\", result_ipcw)\n",
    "print(\"integrated_brier_score\", round(score_brier, 3))\n",
    "mlflow.log_metric(\"integrated_brier_score\", score_brier)\n",
    "print(\"mean_cumulative_dynamic_auc\", round(mean_auc, 3))\n",
    "mlflow.log_metric(\"mean_cumulative_dynamic_auc\", mean_auc)\n",
    "mlflow.log_figure(fig, \"time_dependent_auc.png\")\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training Set: Conducting feature importance analysis on the training set provides insights into the features that the model has learned. However, this approach may not offer an accurate representation of how these features generalize to new data. This allows to use the features as input for other models without leakage.\n",
    "\n",
    "- Test Set: Analyzing feature importance on the test set offers an understanding of feature performance on unseen data. Nonetheless, the reliability of this method may be compromised if the test set is small and imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rsf_t1 = permutation_importance(\n",
    "    rsf_best_t1, X_train, y_train, n_repeats=50, n_jobs=-1, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Crate dataframe\n",
    "df_importance_rsf_t1 = pd.DataFrame(\n",
    "    {\n",
    "        k: result_rsf_t1[k]\n",
    "        for k in (\n",
    "            \"importances_mean\",\n",
    "            \"importances_std\",\n",
    "        )\n",
    "    },\n",
    "    index=X_train.columns,\n",
    ").sort_values(by=\"importances_mean\", ascending=False)\n",
    "del result_rsf_t1\n",
    "\n",
    "# Get definition from dictionary\n",
    "df_importance_rsf_t1[\"feature_definition\"] = df_importance_rsf_t1.index\n",
    "df_importance_rsf_t1[\"feature_definition\"] = df_importance_rsf_t1[\"feature_definition\"].apply(\n",
    "    lambda x: replace_longest_match(x, dict_legend)\n",
    ")\n",
    "mlflow.log_dict(df_importance_rsf_t1.to_dict(), \"df_importance_rsf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(df_importance_rsf_t1, 15, (5, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it better to use 8 levels or 4 levels on `pT`?\n",
    "```\n",
    "mapping_t_4lev = {\n",
    "    \"T1a\": 1.0,\n",
    "    \"T1b\": 1.0,\n",
    "    \"T2a\": 2.0,\n",
    "    \"T2b\": 2.0,\n",
    "    \"T3a\": 3.0,\n",
    "    \"T3b\": 3.0,\n",
    "    \"T3c\": 3.0,\n",
    "    \"T4\": 4.0,\n",
    "    \"Tx\": np.nan,\n",
    "}  # Rare event\n",
    "\n",
    "mapping_t_8lev = {\n",
    "    \"T1a\": 1.0,\n",
    "    \"T1b\": 2.0,\n",
    "    \"T2a\": 3.0,\n",
    "    \"T2b\": 4.0,\n",
    "    \"T3a\": 5.0,\n",
    "    \"T3b\": 6.0,\n",
    "    \"T3c\": 7.0,\n",
    "    \"T4\": 8.0,\n",
    "    \"Tx\": np.nan,\n",
    "}  # Rare event\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_importance_rsf_t1.index.tolist().index(\"pT\"))  # 8 levels\n",
    "print(df_importance_rsf_t1.index.tolist().index(\"pT_4lev\"))  # 4 levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the GRANT features (baseline) are included in the top k features of the Random Survival Forest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1693555061357
    }
   },
   "outputs": [],
   "source": [
    "k = int(len(df_importance_rsf_t1) * 0.25)  # First quartile\n",
    "print(k)\n",
    "# Top k features as list (order of importance)\n",
    "features_selected_raw = df_importance_rsf_t1.index.tolist()[:k]\n",
    "\n",
    "for x in features_grant:\n",
    "    print(x)\n",
    "    if x in features_selected_raw:\n",
    "        print(\n",
    "            \"\\tIncluded in top {} features for Random Survival Forest (rank {})\".format(\n",
    "                k, features_selected_raw.index(x)\n",
    "            )\n",
    "        )\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Survival Forest - T0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"RandomSurvivalForest_selector_T0\"\n",
    "mlflow.start_run(run_name=model_name, nested=True)\n",
    "mlflow.log_param(\"random_state\", RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to obtain a ranking of important features at t0 to gain medical knowledge, and use it as imput to a Cox model (feature selection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features from the legend\n",
    "temp_0 = df_legend[df_legend[\"Domain\"] == \"clinical history\"][\"Variable\"].tolist()\n",
    "\n",
    "# Check if the columns in legend are present in actual dataframe\n",
    "features_t0 = []\n",
    "for feature in temp_0:\n",
    "    for col in df_ohe:\n",
    "        # Check if the actual column starts with the feature (followed by an underscore or nothing)\n",
    "        if col.startswith(f\"{feature}_\") or col == feature:\n",
    "            features_t0.append(col)\n",
    "del temp_0\n",
    "print(len(features_t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter search with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search spaces\n",
    "grid = {\n",
    "    \"n_estimators\": (10, 100),\n",
    "    \"max_depth\": (2, 50),\n",
    "    \"min_samples_split\": (2, 50),\n",
    "    \"min_samples_leaf\": (2, 50),\n",
    "    \"max_features\": (8, min(50, len(features_t0))),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter search with Optuna\n",
    "optimize_rsf(X_tune=X_train[features_t0],\n",
    "             y_tune=y_train,\n",
    "             grid=grid,\n",
    "             n_trials=1000,\n",
    "             n_folds=10,\n",
    "             model_dir=DIR_MODEL_JSON,\n",
    "             model_filename=\"larcher_{}_{}.json\".format(model_name, RANDOM_STATE),\n",
    "             random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "file_path = os.path.join(\n",
    "    DIR_MODEL_JSON, \"larcher_{}_{}.json\".format(model_name, RANDOM_STATE)\n",
    ")\n",
    "\n",
    "# Read the JSON file to get the best hyperparameters\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    print(\"Log from best RandomSurvivalForest model:\")\n",
    "    pprint.pprint(data)\n",
    "    mlflow.log_params(data)\n",
    "    print()\n",
    "    print(\"Grid:\")\n",
    "    pprint.pprint(grid)\n",
    "    assert data[\"random_state\"] == RANDOM_STATE\n",
    "    mlflow.log_params(grid)\n",
    "\n",
    "# Create a new RandomSurvivalForest instance with the best parameters\n",
    "rsf_best_t0 = RandomSurvivalForest(**data[\"best_params\"], random_state=RANDOM_STATE)\n",
    "\n",
    "# Fit the model on the complete training data\n",
    "rsf_best_t0.fit(X_train[features_t0], y_train)\n",
    "if len(rsf_best_t0.feature_names_in_) < 50:\n",
    "    mlflow.log_param(\"feature_names_in\", rsf_best_t0.feature_names_in_)\n",
    "else:\n",
    "    mlflow.log_param(\"feature_names_in\", \"More than 50 features\")\n",
    "mlflow.log_param(\"n_features_in\", rsf_best_t0.n_features_in_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_censored, result_ipcw, score_brier, mean_auc, fig = validate_sksurv_model(model=rsf_best_t0,\n",
    "                                                                                 y_train=y_train,\n",
    "                                                                                 X_test=X_test[features_t0],\n",
    "                                                                                 y_test=y_test,\n",
    "                                                                                 tau=60)\n",
    "print(\"concordance_index_censored\", round(result_censored, 3))\n",
    "mlflow.log_metric(\"concordance_index_censored\", result_censored)\n",
    "print(\"concordance_index_ipcw\", round(result_ipcw, 3))\n",
    "mlflow.log_metric(\"concordance_index_ipcw\", result_ipcw)\n",
    "print(\"integrated_brier_score\", round(score_brier, 3))\n",
    "mlflow.log_metric(\"integrated_brier_score\", score_brier)\n",
    "print(\"mean_cumulative_dynamic_auc\", round(mean_auc, 3))\n",
    "mlflow.log_metric(\"mean_cumulative_dynamic_auc\", mean_auc)\n",
    "mlflow.log_figure(fig, \"time_dependent_auc.png\")\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rsf_t0 = permutation_importance(\n",
    "    rsf_best_t0,\n",
    "    X_train[features_t0],\n",
    "    y_train,\n",
    "    n_repeats=50,\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "# Crate dataframe\n",
    "df_importance_rsf_t0 = pd.DataFrame(\n",
    "    {\n",
    "        k: result_rsf_t0[k]\n",
    "        for k in (\n",
    "            \"importances_mean\",\n",
    "            \"importances_std\",\n",
    "        )\n",
    "    },\n",
    "    index=X_train[features_t0].columns,\n",
    ").sort_values(by=\"importances_mean\", ascending=False)\n",
    "del result_rsf_t0\n",
    "\n",
    "# Get definition from dictionary\n",
    "df_importance_rsf_t0[\"feature_definition\"] = df_importance_rsf_t0.index\n",
    "df_importance_rsf_t0[\"feature_definition\"] = df_importance_rsf_t0[\n",
    "    \"feature_definition\"\n",
    "].apply(lambda x: replace_longest_match(x, dict_legend))\n",
    "mlflow.log_dict(df_importance_rsf_t0.to_dict(), \"df_importance_rsf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(df_importance_rsf_t0, 15, (5, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End mlflow run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernel_info": {
   "name": "uc2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
